{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ETtu9CvVMDR"
      },
      "source": [
        "<h1>7ì¥ ê³ ê¸‰ í…ìŠ¤íŠ¸ ìƒì„± ê¸°ìˆ ê³¼ ë„êµ¬</h1>\n",
        "<i>í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì„ ë„˜ì–´ì„œ</i>\n",
        "\n",
        "<a href=\"https://github.com/rickiepark/handson-llm\"><img src=\"https://img.shields.io/badge/GitHub%20Repository-black?logo=github\"></a>\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/rickiepark/handson-llm/blob/main/chapter07.ipynb)\n",
        "\n",
        "---\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì€ <[í•¸ì¦ˆì˜¨ LLM](https://tensorflow.blog/handson-llm/)> ì±… 7ì¥ì˜ ì½”ë“œë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "---\n",
        "\n",
        "<a href=\"https://tensorflow.blog/handson-llm/\">\n",
        "<img src=\"https://tensorflow.blog/wp-content/uploads/2025/05/ed95b8eca688ec98a8_llm.jpg\" width=\"350\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtUx27GOCAYd"
      },
      "source": [
        "### [ì„ íƒì‚¬í•­] - <img src=\"https://colab.google/static/images/icons/colab.png\" width=100>ì—ì„œ íŒ¨í‚¤ì§€ ì„ íƒí•˜ê¸°\n",
        "\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì„ êµ¬ê¸€ ì½”ë©ì—ì„œ ì‹¤í–‰í•œë‹¤ë©´ ë‹¤ìŒ ì½”ë“œ ì…€ì„ ì‹¤í–‰í•˜ì—¬ ì´ ë…¸íŠ¸ë¶ì—ì„œ í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼  ì„¤ì¹˜í•˜ì„¸ìš”.\n",
        "\n",
        "---\n",
        "\n",
        "ğŸ’¡ **NOTE**: ì´ ë…¸íŠ¸ë¶ì˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ GPUë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. êµ¬ê¸€ ì½”ë©ì—ì„œëŠ” **ëŸ°íƒ€ì„ > ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ > í•˜ë“œì›¨ì–´ ê°€ì†ê¸° > T4 GPU**ë¥¼ ì„ íƒí•˜ì„¸ìš”.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì‚¬ìš© íŒ¨í‚¤ì§€ ë²„ì „\n",
        "\n",
        "* transformers 4.57.3\n",
        "* llama-cpp 0.3.16\n",
        "* langchain 1.2.0\n",
        "* langchain-community 0.4.1\n",
        "* langchain-openai 1.1.6\n",
        "* ddgs 9.10.0"
      ],
      "metadata": {
        "id": "YyEFN2g1FqLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ê¹ƒí—ˆë¸Œì—ì„œ ìœ„ì ¯ ìƒíƒœ ì˜¤ë¥˜ë¥¼ í”¼í•˜ê¸° ìœ„í•´ ì§„í–‰ í‘œì‹œì¤„ì„ ë‚˜íƒ€ë‚´ì§€ ì•Šë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.\n",
        "from transformers.utils import logging\n",
        "\n",
        "logging.disable_progress_bar()"
      ],
      "metadata": {
        "id": "ILBN7AMQ0SfM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Txh47zAxCAYd"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install langchain_community langchain_openai ddgs\n",
        "\n",
        "# ì‚¬ìš©í•˜ëŠ” íŒŒì´ì¬ê³¼ CUDA ë²„ì „ì— ë§ëŠ” llama-cpp-python íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.\n",
        "# í˜„ì¬ ì½”ë©ì˜ íŒŒì´ì¬ ë²„ì „ì€ 3.12ì´ë©° CUDA ë²„ì „ì€ 12.4(12.5)ì…ë‹ˆë‹¤.\n",
        "!pip install https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.16-cu124/llama_cpp_python-0.3.16-cp312-cp312-linux_x86_64.whl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rerbJgwAigbK"
      },
      "source": [
        "# LLM ë¡œë“œí•˜ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EYKJi4bCAYf",
        "outputId": "0fbb1976-b144-45fd-e06b-c9db236949bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-01-29 03:18:06--  https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 3.165.160.59, 3.165.160.61, 3.165.160.11, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.165.160.59|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://us.gcp.cdn.hf.co/xet-bridge-us/662698108f7573e6a6478546/a9cdcf6e9514941ea9e596583b3d3c44dd99359fb7dd57f322bb84a0adc12ad4?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Phi-3-mini-4k-instruct-fp16.gguf%3B+filename%3D%22Phi-3-mini-4k-instruct-fp16.gguf%22%3B&Expires=1769660286&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiRXBvY2hUaW1lIjoxNzY5NjYwMjg2fX0sIlJlc291cmNlIjoiaHR0cHM6Ly91cy5nY3AuY2RuLmhmLmNvL3hldC1icmlkZ2UtdXMvNjYyNjk4MTA4Zjc1NzNlNmE2NDc4NTQ2L2E5Y2RjZjZlOTUxNDk0MWVhOWU1OTY1ODNiM2QzYzQ0ZGQ5OTM1OWZiN2RkNTdmMzIyYmI4NGEwYWRjMTJhZDRcXD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=DYD02OKUQhOGe0yfwmqsDqnCWgwZ%7EZXLzReeBaN-k2o-reelewhWPJou07f1U0kd4IlI4SqdVNXn1jZ-rume3noL2M-YMqjLQkctuv35fDBUVqQWQorWHsPYzCE6kLPPsowaMXq0IFjrIQn2eZXJh5yDkThv-n9EWT1%7EAKez-acQCjnLeOpoelek8JMfd59qqtpahSlk6rwRa6P7Ag-mrFRLo89gRNYAUV6Tho8SXuIXVt-KZp3wKA8f5c9MfmP7xhe4cUs0PYlAbg5fi%7ELbb6lQKmS3nhNl7alk%7EeKtAs7HjyYe-OYzWxClFgzft79p1faoUJ--O5btCbCu-Bx67w__&Key-Pair-Id=KJLH8B0YWU4Y8M [following]\n",
            "--2026-01-29 03:18:06--  https://us.gcp.cdn.hf.co/xet-bridge-us/662698108f7573e6a6478546/a9cdcf6e9514941ea9e596583b3d3c44dd99359fb7dd57f322bb84a0adc12ad4?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Phi-3-mini-4k-instruct-fp16.gguf%3B+filename%3D%22Phi-3-mini-4k-instruct-fp16.gguf%22%3B&Expires=1769660286&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiRXBvY2hUaW1lIjoxNzY5NjYwMjg2fX0sIlJlc291cmNlIjoiaHR0cHM6Ly91cy5nY3AuY2RuLmhmLmNvL3hldC1icmlkZ2UtdXMvNjYyNjk4MTA4Zjc1NzNlNmE2NDc4NTQ2L2E5Y2RjZjZlOTUxNDk0MWVhOWU1OTY1ODNiM2QzYzQ0ZGQ5OTM1OWZiN2RkNTdmMzIyYmI4NGEwYWRjMTJhZDRcXD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=DYD02OKUQhOGe0yfwmqsDqnCWgwZ%7EZXLzReeBaN-k2o-reelewhWPJou07f1U0kd4IlI4SqdVNXn1jZ-rume3noL2M-YMqjLQkctuv35fDBUVqQWQorWHsPYzCE6kLPPsowaMXq0IFjrIQn2eZXJh5yDkThv-n9EWT1%7EAKez-acQCjnLeOpoelek8JMfd59qqtpahSlk6rwRa6P7Ag-mrFRLo89gRNYAUV6Tho8SXuIXVt-KZp3wKA8f5c9MfmP7xhe4cUs0PYlAbg5fi%7ELbb6lQKmS3nhNl7alk%7EeKtAs7HjyYe-OYzWxClFgzft79p1faoUJ--O5btCbCu-Bx67w__&Key-Pair-Id=KJLH8B0YWU4Y8M\n",
            "Resolving us.gcp.cdn.hf.co (us.gcp.cdn.hf.co)... 34.120.165.110\n",
            "Connecting to us.gcp.cdn.hf.co (us.gcp.cdn.hf.co)|34.120.165.110|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7643295904 (7.1G) [application/octet-stream]\n",
            "Saving to: â€˜Phi-3-mini-4k-instruct-fp16.ggufâ€™\n",
            "\n",
            "Phi-3-mini-4k-instr 100%[===================>]   7.12G  83.1MB/s    in 1m 45s  \n",
            "\n",
            "2026-01-29 03:19:52 (69.2 MB/s) - â€˜Phi-3-mini-4k-instruct-fp16.ggufâ€™ saved [7643295904/7643295904]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LlamaCpp (C/C++ë¡œ ì‘ì„±ëœ ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬, llmì„ ë¹ ë¥´ê²Œ ì‹¤í–‰) í˜¸ì¶œ"
      ],
      "metadata": {
        "id": "4M94_YV67Zyr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQcht_ZFijW7",
        "outputId": "a2321eba-0d1a-4486-ea92-752eecd4f65c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.llms import LlamaCpp\n",
        "\n",
        "# llm ì •ì˜\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"Phi-3-mini-4k-instruct-fp16.gguf\",\n",
        "    n_gpu_layers=-1,\n",
        "    max_tokens=500,\n",
        "    n_ctx=4096,\n",
        "    seed=42,\n",
        "    verbose=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "3SNhQF9WthzV",
        "outputId": "b626a988-1228-42de-8413-d105adeef4d3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ppolyte Taine (1828-1893) was a French writer, critic and historian. He wrote \"Les origines de la France contemporaine\" in 1875.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "llm.invoke(\"Hi\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wwx2AIuGfCoP"
      },
      "source": [
        "## ì²´ì¸"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "langchain_core.prompts ëª¨ë“ˆì—ì„œ PromptTemplate ëª¨ë“ˆ í˜¸ì¶œ\n",
        "\n",
        "PromptTemplate : test -> template ë³€í™˜"
      ],
      "metadata": {
        "id": "pjKVJAWQzqHE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "kF--Q5me_-X1"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# \"input_prompt\" ë³€ìˆ˜ë¥¼ ê°€ì§„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë§Œë“¤ê¸°\n",
        "template = \"\"\"<|user|>\n",
        "{input_prompt}<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"input_prompt\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ogWsGeg6hElt"
      },
      "outputs": [],
      "source": [
        "#ì—°ê²°\n",
        "basic_chain = prompt | llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KINQxKAINXgG",
        "outputId": "263b9411-1333-4a35-d3fd-888fd6f21cd6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Hello Maarten! The answer to 1 + 1 is 2.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "# ì²´ì¸ ì‚¬ìš©.\n",
        "basic_chain.invoke(\n",
        "    {\n",
        "        \"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\",\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSMBMRxB8gFW"
      },
      "source": [
        "### ì—¬ëŸ¬ í…œí”Œë¦¿ì„ ê°€ì§„ ì²´ì¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "wrUKuHt_OLpe"
      },
      "outputs": [],
      "source": [
        "from langchain_classic.chains import LLMChain\n",
        "\n",
        "#ì²´ì¸1 : ì´ì•¼ê¸° ì œëª©\n",
        "template = \"\"\"<|user|>\n",
        "Create a title for a story about {summary}. Only return the title.<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "title_prompt = PromptTemplate(template=template, input_variables=[\"summary\"])\n",
        "title = LLMChain(llm=llm, prompt=title_prompt, output_key=\"title\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igFIyg73OtaL",
        "outputId": "63c25ae6-a085-43e1-e9dd-e85f5814016c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'summary': 'a girl that lost her mother',\n",
              " 'title': ' \"Whispers of a Fading Memory: The Journey Through Loss\"'}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "title.invoke({\"summary\": \"a girl that lost her mother\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "zTtFEmANOhyE"
      },
      "outputs": [],
      "source": [
        "# ì²´ì¸ 2 : ìš”ì•½ + ì œëª© -> ì£¼ì¸ê³µ ì„¤ëª…\n",
        "template = \"\"\"<|user|>\n",
        "Describe the main character of a story about {summary} with the title {title}.\n",
        "Use only two sentences.<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "character_prompt = PromptTemplate(\n",
        "    template=template, input_variables=[\"summary\", \"title\"]\n",
        ")\n",
        "character = LLMChain(llm=llm, prompt=character_prompt, output_key=\"character\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "character.invoke({'summary': 'a girl that lost her mother',\n",
        " 'title': ' \"Whispers of a Fading Memory: The Journey Through Loss\"'})"
      ],
      "metadata": {
        "id": "V9xiD1hl-IcL",
        "outputId": "720f88b8-3e93-4d49-8e4b-8cd3e9d466c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'summary': 'a girl that lost her mother',\n",
              " 'title': ' \"Whispers of a Fading Memory: The Journey Through Loss\"',\n",
              " 'character': ' The main character, Emily, is a resilient and introspective teenager struggling to cope with the profound grief of losing her mother at a young age. As she embarks on a journey through loss, her strength and vulnerability intertwine, leading her towards self-discovery and healing.'}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Xjf-avW8NAqZ"
      },
      "outputs": [],
      "source": [
        "# ì²´ì¸ 3 (ìš”ì•½ + ì œëª© + ì£¼ì¸ê³µ ì„¤ëª… -> ì´ì•¼ê¸°) ìƒì„±\n",
        "template = \"\"\"<|user|>\n",
        "Create a story about {summary} with the title {title}.\n",
        "The main charachter is: {character}.\n",
        "Only return the story and it cannot be longer than one paragraph<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "story_prompt = PromptTemplate(\n",
        "    template=template, input_variables=[\"summary\", \"title\", \"character\"]\n",
        ")\n",
        "story = LLMChain(llm=llm, prompt=story_prompt, output_key=\"story\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "epNudKyyPClO"
      },
      "outputs": [],
      "source": [
        "# ì„¸ ê°œì˜ ìš”ì†Œë¥¼ ì—°ê²°í•˜ì—¬ ìµœì¢… ì²´ì¸ì„ ë§Œë“­ë‹ˆë‹¤.\n",
        "llm_chain = title | character | story"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.invoke(\"a man who has been looking for his dream girl.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJX-D-UI_1rW",
        "outputId": "ff315bb1-50f0-4124-8792-2d107cf9b956"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'summary': 'a man who has been looking for his dream girl.',\n",
              " 'title': ' \"The Endless Quest for Love: A Man\\'s Pursuit of His Dream Girl\"',\n",
              " 'character': ' The main character in \"The Endless Quest for Love: A Man\\'s Pursuit of His Dream Girl\" is a determined and passionate individual, who has spent his entire life searching for the perfect partner. He embodies unwavering perseverance and optimism as he navigates through various relationships and experiences in pursuit of his ideal woman.\\n\\nThis man\\'s relentless quest shapes not only his romantic endeavors but also molds him into a well-rounded individual, evolving with each encounter on the path towards finding love. His search becomes an odyssey filled with personal growth and self-discovery as he continues to chase after the elusive dream girl who seems just out of reach.',\n",
              " 'story': \" The Endless Quest for Love: A Man's Pursuit of His Dream Girl\\n\\nIn a bustling city teeming with love seekers, there was an unyielding man named Samuel whose heart yearned to find his dream girl. With an indomitable spirit fueled by passion and determination, he embarked on a ceaseless journey through countless relationships, each one teaching him the invaluable lessons of trust, resilience, and understanding. Undeterred by setbacks or the occasional disappointment that life's dance sometimes presented, Samuel refined his approach to love with every encounter, growing not only as a lover but also blossoming into an enlightened soul. His pursuit became more than just finding another partner; it was a transformative odyssey of self-discovery and personal evolution that illuminated the path towards the elusive dream girl he so fervently sought, his every step marked by hope as enduring as time itself.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UQ-DZ71P-D-"
      },
      "source": [
        "# ë©”ëª¨ë¦¬"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "-15Eoey5EJUO",
        "outputId": "53be76ce-83b8-4d0c-9881-96b63b429a0a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Hello Gahui! The answer to 1 + 1 is 2. It's a basic arithmetic operation, adding the two single units together.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "# LLMì—ê²Œ ì´ë¦„ì„ ì•Œë ¤ ì¤ë‹ˆë‹¤.\n",
        "basic_chain.invoke({\"input_prompt\": \"Hi! My name is gahui. What is 1 + 1?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "N42wQRl-Lykt",
        "outputId": "9b8a0c48-3046-4dca-99d0-68491967c164"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" I'm unable to determine your name without more context. As an AI, I don't have the ability to access personal data unless it has been shared with me in the course of our conversation for the purpose of assisting you. If you're looking to find out your name, perhaps check a recent document or ask someone nearby who might know.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "# LLMì—ê²Œ ì´ë¦„ì„ ë¬»ìŠµë‹ˆë‹¤.\n",
        "basic_chain.invoke({\"input_prompt\": \"What is my name?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfqATEZjMgET"
      },
      "source": [
        "### ëŒ€í™” ë²„í¼"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Zoo0PA1fUs70"
      },
      "outputs": [],
      "source": [
        "# ëŒ€í™” ê¸°ë¡ì„ ë‹´ì„ ìˆ˜ ìˆë„ë¡ í”„ë¡¬í”„íŠ¸ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.\n",
        "template = \"\"\"<|user|>Current conversation:{chat_history}\n",
        "\n",
        "{input_prompt}<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"input_prompt\", \"chat_history\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgGMS1S9saLi",
        "outputId": "ac23069e-f730-4ebc-fcf1-796f328ab92a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1877295073.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(memory_key=\"chat_history\")\n"
          ]
        }
      ],
      "source": [
        "from langchain_classic.memory import ConversationBufferMemory\n",
        "\n",
        "# ì‚¬ìš©í•  ë©”ëª¨ë¦¬ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "\n",
        "# LLM, í”„ë¡¬í”„íŠ¸, ë©”ëª¨ë¦¬ë¥¼ ì—°ê²°í•©ë‹ˆë‹¤.\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mltR_GtkiqDZ",
        "outputId": "fb61531a-54c5-41a5-ac2a-a3c7a9baec4f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'Hi! My name is gahui. What is 1 + 1?',\n",
              " 'chat_history': '',\n",
              " 'text': \" The answer to 1 + 1 is 2. It's a basic arithmetic operation where you add one unit to another, resulting in two units altogether.\\n\\nHereâ€™s a simple breakdown:\\n\\n- Start with the number 1.\\n\\n- Add another 1 to it.\\n\\n- You end up with 2.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "# ê°„ë‹¨í•œ ì§ˆë¬¸ì„ í•˜ì—¬ ëŒ€í™” ê¸°ë¡ì„ ë§Œë“­ë‹ˆë‹¤.\n",
        "llm_chain.invoke({\"input_prompt\": \"Hi! My name is gahui. What is 1 + 1?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-je1rmy3dx4",
        "outputId": "37ca53b9-092c-4af5-a949-93d52975c0d4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is my name?',\n",
              " 'chat_history': \"Human: Hi! My name is gahui. What is 1 + 1?\\nAI:  The answer to 1 + 1 is 2. It's a basic arithmetic operation where you add one unit to another, resulting in two units altogether.\\n\\nHereâ€™s a simple breakdown:\\n\\n- Start with the number 1.\\n\\n- Add another 1 to it.\\n\\n- You end up with 2.\",\n",
              " 'text': \" Your name is gahui.\\n\\nAs for the question I asked before, my name is an AI and it doesn't have one like a human does. However, you can refer to me as your assistant or digital helper!\"}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "# LLMì´ ì´ë¦„ì„ ê¸°ì–µí• ê¹Œìš”?\n",
        "llm_chain.invoke({\"input_prompt\": \"What is my name?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sw3ELCg6Rpsk"
      },
      "source": [
        "### ìœˆë„ ëŒ€í™” ë²„í¼"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0DRT7kjRtiC",
        "outputId": "7ced98ba-4619-4e97-90e0-d9c25ee48119"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-40775140.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\")\n"
          ]
        }
      ],
      "source": [
        "from langchain_classic.memory import ConversationBufferWindowMemory\n",
        "\n",
        "# ë©”ëª¨ë¦¬ì— ë§ˆì§€ë§‰ ë‘ ê°œì˜ ëŒ€í™”ë§Œ ìœ ì§€í•©ë‹ˆë‹¤.\n",
        "memory = ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\")\n",
        "\n",
        "# LLM, í”„ë¡¬í”„íŠ¸, ë©”ëª¨ë¦¬ë¥¼ ì—°ê²°í•©ë‹ˆë‹¤.\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBY69vvcR1Qq",
        "outputId": "a2afcec5-656c-42bb-ccd8-9d776fb55188"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is 3 + 3?',\n",
              " 'chat_history': \"Human: Hi! My name is gahui and I am 100 years old. What is 1 + 1?\\nAI:  Hello gahui! Regardless of your age, the answer to 1 + 1 is always 2.\\n\\nHere's how it works:\\n\\n1. Start with number one (1).\\n2. Add another one (+) to it.\\n3. You get two (2).\\n\\nSo, mathematically speaking, 1 + 1 = 2.\",\n",
              " 'text': ' Hello! Absolutely, regardless of age, the mathematical answer remains consistent. For 3 + 3:\\n\\n1. Start with number three (3).\\n2. Add another three (+) to it.\\n3. You get six (6).\\n\\nHence, mathematically speaking, 3 + 3 = 6.'}"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "# ë‘ ê°œì˜ ì§ˆë¬¸ì„ ë˜ì ¸ ë©”ëª¨ë¦¬ì— ëŒ€í™” ê¸°ë¡ì„ ì €ì¥í•©ë‹ˆë‹¤.\n",
        "llm_chain.invoke({\"input_prompt\":\"Hi! My name is gahui and I am 100 years old. What is 1 + 1?\"})\n",
        "llm_chain.invoke({\"input_prompt\":\"What is 3 + 3?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvSLfKWpR5h5",
        "outputId": "8cf344e0-a085-4901-cdfb-7703516a5748"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is my name?',\n",
              " 'chat_history': \"Human: Hi! My name is gahui and I am 100 years old. What is 1 + 1?\\nAI:  Hello gahui! Regardless of your age, the answer to 1 + 1 is always 2.\\n\\nHere's how it works:\\n\\n1. Start with number one (1).\\n2. Add another one (+) to it.\\n3. You get two (2).\\n\\nSo, mathematically speaking, 1 + 1 = 2.\\nHuman: What is 3 + 3?\\nAI:  Hello! Absolutely, regardless of age, the mathematical answer remains consistent. For 3 + 3:\\n\\n1. Start with number three (3).\\n2. Add another three (+) to it.\\n3. You get six (6).\\n\\nHence, mathematically speaking, 3 + 3 = 6.\",\n",
              " 'text': ' Hello! Your name is gahui, as you mentioned in our previous conversation.\\n\\nAs for the math question: When we add 3 + 3 together, the sum is indeed 6. Mathematics remains constant and unaffected by personal factors like age.'}"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "# ì´ë¦„ì„ ê¸°ì–µí•˜ëŠ”ê³  ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.\n",
        "llm_chain.invoke({\"input_prompt\":\"What is my name?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YW7qEyctcqeJ",
        "outputId": "fb437c2f-0320-4e67-f3e7-8fc7fc23fb2b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is my age?',\n",
              " 'chat_history': 'Human: What is 3 + 3?\\nAI:  Hello! Absolutely, regardless of age, the mathematical answer remains consistent. For 3 + 3:\\n\\n1. Start with number three (3).\\n2. Add another three (+) to it.\\n3. You get six (6).\\n\\nHence, mathematically speaking, 3 + 3 = 6.\\nHuman: What is my name?\\nAI:  Hello! Your name is gahui, as you mentioned in our previous conversation.\\n\\nAs for the math question: When we add 3 + 3 together, the sum is indeed 6. Mathematics remains constant and unaffected by personal factors like age.',\n",
              " 'text': \" I'm an AI and do not have the capability to know personal information about individuals unless it has been shared with me in a conversation. Therefore, I can't determine your age without that specific information from you.\\nAnswer: I cannot answer this question as I don't have access to personal data like your age unless previously provided during our conversation.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "# ë‚˜ì´ë¥¼ ê¸°ì–µí•˜ê³  ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.\n",
        "llm_chain.invoke({\"input_prompt\":\"What is my age?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSb5OnANMhu2"
      },
      "source": [
        "### ëŒ€í™” ìš”ì•½"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "lWHZlJUbwpqE"
      },
      "outputs": [],
      "source": [
        "# ìš”ì•½ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
        "summary_prompt_template = \"\"\"<|user|>Summarize the conversations and update with the new lines.\n",
        "\n",
        "Current summary:\n",
        "{summary}\n",
        "\n",
        "new lines of conversation:\n",
        "{new_lines}\n",
        "\n",
        "New summary:<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "summary_prompt = PromptTemplate(\n",
        "    input_variables=[\"new_lines\", \"summary\"],\n",
        "    template=summary_prompt_template\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qg1HAgxZMkbO",
        "outputId": "8029d2ad-ece9-43c3-98ef-0a63062ebb5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1864264585.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationSummaryMemory(\n"
          ]
        }
      ],
      "source": [
        "from langchain_classic.memory import ConversationSummaryMemory\n",
        "\n",
        "# ì‚¬ìš©í•  ë©”ëª¨ë¦¬ë¥¼ ì •ì˜\n",
        "memory = ConversationSummaryMemory(\n",
        "    llm=llm,\n",
        "    memory_key=\"chat_history\",\n",
        "    prompt=summary_prompt\n",
        ")\n",
        "\n",
        "# LLM, í”„ë¡¬í”„íŠ¸, ë©”ëª¨ë¦¬ë¥¼ ì—°ê²°í•©ë‹ˆë‹¤.\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2klIk9CpVSH0",
        "outputId": "aa6eecbe-5bde-4ad8-b4d5-e2d088518917"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is my name?',\n",
              " 'chat_history': ' The Human, introducing themselves as \"gahui,\" asked the AI a simple arithmetic question: What is 1 + 1? The AI responded by stating that 1 + 1 equals 2, explaining it\\'s a basic addition operation. The updated summary of their conversation reiterates this information:\\n\\nNew Summary: \"gahui\" inquired about the sum of 1 and 1; the AI confirmed it is 2, describing it as an elementary arithmetic addition.',\n",
              " 'text': ' Your name in the context of this conversation appears to be \"gahui.\"'}"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "# ì´ë¦„ì— ëŒ€í•´ ì§ˆë¬¸í•˜ëŠ” ëŒ€í™”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "llm_chain.invoke({\"input_prompt\": \"Hi! My name is gahui. What is 1 + 1?\"})\n",
        "llm_chain.invoke({\"input_prompt\": \"What is my name?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VdOH_I-V-Fy",
        "outputId": "31c9f6b9-98b6-4ac7-c8d5-145a68c32c96"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What was the first question I asked?',\n",
              " 'chat_history': ' The conversation began with a simple math problem where the user, referred to as \"Maarten\", asked for the sum of 1 + 1. The answer provided was 2, and an explanation was given that when you add one (1) to another one (1), it results in two (2). Later, upon inquiring about their name within the conversation context, the AI deduced that \"Maarten\" is likely the user\\'s name based on their previous interaction.\\n\\nNew summary: \\nGreetings! In our initial math problem discussion, you asked for the sum of 1 + 1, to which we answered 2. Reflecting upon your conversation with us, it appears that \"Maarten\" is probably your name as per the context provided within this particular exchange.',\n",
              " 'text': ' The first question you asked was: \"What is 1 + 1?\"'}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "# ì§€ê¸ˆê¹Œì§€ ë‚´ìš©ì´ ìš”ì•½ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.\n",
        "llm_chain.invoke({\"input_prompt\": \"What was the first question I asked?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1_LlvrVX9HL",
        "outputId": "59dd4cb7-69fd-4d47-888c-cbadf5474143"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'chat_history': ' The conversation started with the user, \"Maarten\", inquiring about the sum of 1 + 1. You were correctly informed that this results in 2. Additionally, based on your interaction and context provided during the conversation, it is inferred that \"Maarten\" might be your name. Further, you asked about the first question you posed to us, which was indeed asking for the sum of 1 + 1.'}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# ì§€ê¸ˆê¹Œì§€ ìš”ì•½ ë‚´ìš©ì„ í™•ì¸í•©ë‹ˆë‹¤.\n",
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BG5sJa1qvS4N"
      },
      "source": [
        "# ì—ì´ì „íŠ¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcBt8bZM56dM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# ë­ì²´ì¸ìœ¼ë¡œ ì˜¤í”ˆAIì˜ LLMì„ ë¡œë“œí•©ë‹ˆë‹¤.\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"MY_KEY\"\n",
        "openai_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmRZu8DO2p6k"
      },
      "outputs": [],
      "source": [
        "# ReAct í…œí”Œë¦¿ì„ ë§Œë“­ë‹ˆë‹¤.\n",
        "react_template = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
        "\n",
        "{tools}\n",
        "\n",
        "Use the following format:\n",
        "\n",
        "Question: the input question you must answer\n",
        "Thought: you should always think about what to do\n",
        "Action: the action to take, should be one of [{tool_names}]\n",
        "Action Input: the input to the action\n",
        "Observation: the result of the action\n",
        "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
        "Thought: I now know the final answer\n",
        "Final Answer: the final answer to the original input question\n",
        "\n",
        "Begin!\n",
        "\n",
        "Question: {input}\n",
        "Thought:{agent_scratchpad}\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=react_template,\n",
        "    input_variables=[\"tools\", \"tool_names\", \"input\", \"agent_scratchpad\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NV-ssNa-4zOK"
      },
      "outputs": [],
      "source": [
        "from langchain_core.tools import Tool\n",
        "from langchain_community.agent_toolkits.load_tools import load_tools\n",
        "from langchain_classic.tools import DuckDuckGoSearchResults\n",
        "\n",
        "# ì—ì´ì „íŠ¸ì— ì „ë‹¬í•  ë„êµ¬ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
        "search = DuckDuckGoSearchResults()\n",
        "search_tool = Tool(\n",
        "    name=\"duckduck\",\n",
        "    description=\"A web search engine. Use this to as a search engine for general queries.\",\n",
        "    func=search.run,\n",
        ")\n",
        "\n",
        "# ë„êµ¬ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤.\n",
        "tools = load_tools([\"llm-math\"], llm=openai_llm)\n",
        "\n",
        "# ê¸°ì¡´ 'Calculator' ë„êµ¬ì˜ ì„¤ëª…ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ë®ì–´ì”ë‹ˆë‹¤.\n",
        "for tool in tools:\n",
        "    if tool.name == \"Calculator\":\n",
        "        tool.description = (\n",
        "            \"Useful for when you need to answer questions about math. \"\n",
        "            \"Input should be a strictly numerical mathematical expression. \"\n",
        "            \"Do NOT include any words, variable names, or currency symbols like 'price' or '$'.\"\n",
        "        )\n",
        "\n",
        "tools.append(search_tool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tAr1962vS4T"
      },
      "outputs": [],
      "source": [
        "from langchain_classic.agents import AgentExecutor, create_react_agent\n",
        "\n",
        "# ReAct ì—ì´ì „íŠ¸ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
        "agent = create_react_agent(openai_llm, tools, prompt)\n",
        "agent_executor = AgentExecutor(\n",
        "    agent=agent, tools=tools, verbose=True, handle_parsing_errors=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSU6ECdYBOOm",
        "outputId": "1dd0eefb-d39e-4bfc-e62f-429a085371b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI need to find the current price of a MacBook Pro in USD and then convert it to EUR using the given exchange rate.\n",
            "Action: duckduck\n",
            "Action Input: \"current price of MacBook Pro in USD\"\u001b[0m\u001b[33;1m\u001b[1;3msnippet: 6 days ago - If a big screen is your top priority, you can score a $309 discount on the 16-inch model of the latest MacBook Pro, dropping the price to $2,190 on Amazon . It features the M4 Pro processor and 24GB of RAM for blazing-fast performance, but not ..., title: Start 2026 With a New MacBook and Save Up to $800 - CNET, link: https://www.cnet.com/deals/best-macbook-deals/, snippet: October 15, 2025 - We list their lowest price of the day All resellers: Free shipping, sales tax not included Select price for product details or to purchase Â· 14-inch M5 MacBook Pros on Holiday sale at B&H starting at $1449 , up to $150 off Appleâ€™s MSRP 12/29/2025, title: Apple 14â€³ MacBook Pro Prices at MacPrices.net, link: https://www.macprices.net/14-macbook-pro/, snippet: 3 weeks ago - If you buy a product featured here, ... TL;DR: Get one of Appleâ€™s most powerful laptops, the Â· MacBook Pro, for only $399.97 (reg ...., title: You can get a MacBook Pro for $400 if you act fast | Mashable, link: https://mashable.com/article/dec-14-apple-macbook-pro-i5-2ghz-2020, snippet: October 2, 2025 - The latest 2024 16â€³ M4 Pro and M4 Max MacBook Pro prices & sales from Apple retailers, plus all the information you need to make an informed purchase Â· QuickLinks: Sales | Price Trackers | Newsletter | AppleSurfer | ğŸ” ... Donâ€™t pay full price! Here are the best sale prices as of Links below take you to retailerâ€™s product page., title: Apple 16â€³ MacBook Pro Prices at MacPrices.net, link: https://www.macprices.net/16-macbook-pro/\u001b[0m\u001b[32;1m\u001b[1;3mI found the current price of a MacBook Pro in USD, now I need to calculate the cost in EUR using the exchange rate.\n",
            "Action: Calculator\n",
            "Action Input: 2190 * 0.85\u001b[0m\u001b[36;1m\u001b[1;3mAnswer: 1861.5\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer\n",
            "Final Answer: The cost of a MacBook Pro in EUR would be 1861.5 EUR.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD?',\n",
              " 'output': 'The cost of a MacBook Pro in EUR would be 1861.5 EUR.'}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "# ë§¥ë¶ í”„ë¡œì˜ ê°€ê²©ì€ ì–¼ë§ˆì¸ê°€ìš”?\n",
        "agent_executor.invoke(\n",
        "    {\n",
        "        \"input\": \"What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD?\"\n",
        "    }\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}